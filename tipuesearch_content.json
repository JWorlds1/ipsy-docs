{"pages":[{"title":"How to Ask for Help","text":"\"Computers are like Old Testament gods; lots of rules, and no mercy.\" — Joseph Campbell Before you ask someone for help, be sure that you have done the following: If there's an error message: read it. Though sometimes cryptic, all contain information that can help you to understand what's happening. Think about the task you're trying to perform, and as many of the behind-the-scenes steps and parts involved as possible. Consider how the error fits into that stack. Which part of the system is throwing the error: your script, another command, the language, the file system, the network? If the error message isn't immediately helpful, search for it using a search engine. Read the documentation and/or man page. Perhaps what you think you're doing is not actually what the command is doing. If you have considered the above, and both your insight and search-fu are failing you, then it's likely time to ask someone else for help. When asking for help, be sure to provide the person the following information: Clearly describe what you're trying to do Provide the error message (if any) Explain what you've already done to fix/understand this A good rule of thumb is that the amount of effort you put into understanding/solving a problem is the maximum effort that the other person will invest. Put more pointedly: if your description is that \"it doesn't work\", you should fully expect no response — as their response-effort precisely matches the effort you've invested into explaining your problem. When you are met with the answer, make sure that you are engaged with it — and understand it. The goal here is not just to fix your immediate problem. It's to build a body of knowledge and the skill of troubleshooting. This all culminates at the end-goal: computers should be tools which empower you — rather than something inflicted upon you.","tags":"pages","url":"how_to_ask/"},{"title":"Transferring Data","text":"There are numerous methods to share/publish/exchange data with others. Small Files (< 2 GiB) Every IPSY user has a public_html/ folder in their home directory on kumo.ovgu.de . Any files placed in that folder will be made available at http://kumo.ovgu.de/~<username>/ . Larger, One-time Transfers A dedicated account (with a temporary password) on Medusa to exchange larger collections of data with non-Medusa users. rsync is highly recommended, as it checksums every file transferred, guaranteeing that nothing was corrupted during the transfer. Windows users (who do not have access to rsync ) can use WinSCP instead; however, WinSCP does not support checksumming. Publishing Datasets Many large datasets are available for public consumption on psydata.ovgu.de (AKA Mulder). Data are published both via an rsync daemon and on a web server. http://psydata.ovgu.de","tags":"pages","url":"transferring_data/"},{"title":"Medusa","text":"Medusa is a small computational cluster used for neuroscience research by a hardy cohort of scientists at the Institute of Psychology II at the Otto-von-Guericke University of Magdeburg . Maintained by Alex Waite, Medusa is tailored to the analysis needs of psychology researchers — running Debian Linux with additional research software provided by NeuroDebian When you first use Medusa, you will use one machine: the head node. However, Medusa is a collection of servers, with many dedicated computational nodes. In order to use Medusa to its full potential, you will need to become familiar with our job scheduler, Condor .","tags":"pages","url":"medusa/"},{"title":"Experimental Labs","text":"","tags":"pages","url":"labs/"},{"title":"Code of Conduct","text":"Sharing is caring. —Wise Person Medusa is a shared resource, and thus should be used with awareness and empathy for the work of others. Specific points to pay attention to are: Use Condor for analysis The head node is meant for interactive use and quick computations. Users should use no more than 1 or 2 cores at a time. Anything more than that actively interferes with other people's work. Condor should be used for all non-trivial computation — and will give you results faster than just using the head node. Be mindful of your storage space Treat storage space as if it's a finite resource (pro-tip: it is). Take the time to regularly remove obsolete data and temporary files. Temporary/easily-regeneratable data should be stored in a scratch/ directory. More information is available in the Data Documentation . Report anything strange/faulty If you notice something broken (or even just strange) while working with Medusa, take the time to report it to Alex or Michael. If something isn't right, it likely affects others too.","tags":"medusa","url":"medusa/code_of_conduct/"},{"title":"Accessing Medusa","text":"Command Line The easiest and most reliable way of connecting to Medusa is via ssh . Linux and macOS Connecting is as simple as running the following in your terminal ssh username@medusa.ovgu.de To run graphical programs over SSH and have them display locally, you can use the -X option. However, remote X is very sensitive to latency, so it is only practical while on OvGU's campus. ssh -X username@medusa.ovgu.de macOS users need XQuartz [1] installed in order to use the -X option. Passwordless authentication to Medusa is possible through the use of SSH keys. If you do not have an SSH key yet, GitHub has some good instructions on generating a key and (optionally) adding it to an ssh-agent. Once you have an SSH key, you can easily copy the public key to Medusa: ssh-copy-id username@medusa.ovgu.de Windows Windows lacks built-in support for SSH, so you need to download PuTTY . Open PuTTY, enter medusa.ovgu.de as the host, and select 'SSH' as the login protocol. Click 'open' to establish a secure connection. Enter your username and password, and you're ready to go. Running graphical programs over SSH on Windows is possible (using VcXsrv ), but this method is deprecated and VNC is strongly recommended instead. VNC (Graphical Desktop) VNC is for users who prefer a more familiar desktop experience or need to use graphical programs while off-campus. VNC is a multi-step process and not as easy as straight SSH. First, SSH into Medusa (explained above) to 1) setup your VNC password and 2) start your VNC server. You only need to set your password once. It's stored unencrypted in a text file, so do not use a valuable password (such as for Medusa, email, etc). me@medusa:~$ vncpasswd Password: Verify: Next, start the VNC server. me@medusa:~$ vncserver New 'medusa:9 (me)' desktop is medusa:9 Starting applications specified in /home/me/.vnc/xstartup Log file is /home/me/.vnc/medusa:9.log Note the number 9 in medusa:9 . Yours will likely be different. Take note of your number, as it references your VNC session and will be used later. The VNC server runs until it is terminated by you (or a reboot of Medusa). If you close your client/disconnect, it will continue to run. If you \"log out\" in the session, it will terminate the server. Linux Open a terminal and run the following: vncviewer -via medusa.ovgu.de :9 Note the :9 . Make sure that this is your number from above. This command will first ask that you authenticate to Medusa, and then it will ask for the VNC password you set before. macOS macOS has a VNC client built-in, but it isn't SSH aware. So a SSH tunnel needs to be setup first. ssh -f -L 5909:127.0.0.1:5909 username@medusa.ovgu.de sleep 60 Then you can use VNC. open vnc://127.0.0.1:5909 Note the number \"5909\" in both commands. It should be 5900 + the number noted above. Make sure that it is your number. TODO : write a simple shell script that can be curl -ed into the user's path. Windows Windows lacks a built-in VNC viewer, so you will need TightVNC Java Viewer . Be sure to download the TightVNC Java Viewer ; it is not the first link on their download page. You will also need Java installed in order to run it. Launch the viewer and enter the following information: Remote Host: 127.0.0.1 Port: 5900 + your unique session number (e.g. 5909) Check \"Use SSH Tunneling\" SSH server: medusa.ovgu.de SSH port: 22 SSH User: <your username> Note the port number: \"5909\". It should be 5900 + the number we noted above. Make sure that it is your number. Upon connecting, you will first be asked to authenticate to Medusa; then it will ask for the VNC password you set before. Footnotes [1] There have been some problems with recent XQuartz releases, but users have reported that version 2.7.7 works best for them. It is recommended to use that version until 2.7.12 is released.","tags":"medusa","url":"medusa/accessing_medusa/"},{"title":"G23-R010: EEG","text":"This lab is operated by the Ullsperger Lab. For questions, problems, or scheduling, contact Christina Becker or Laura Waite . Overview The lab has two soundproof cabins with Faraday cages for EEG experiments. Each cabin has its own KVM matrix, which allows for nearly complete flexibility over which computer (EEG recorder, Eye Tracker, Presentation Machine, or researcher supplied Laptop) displays to any screen — and routes both audio and USB. The KVM matrices are also interconnected, so one machine can display to both cabins. Cabin 1 (left) EEG; speakers; eye tracking Cabin 2 (right) EEG; pain stimulation; Current Designs peripherals Software The Presentation Computers run Windows 7 and have been configured in compliance with Brain Products' official guidelines. These machines should never, under any circumstances, be connected to the network. Both machines run Matlab (2012b) and Presentation (multiple versions). EEG Brain Products is the EEG equipment vendor used in this laboratory. Cabin 1 (left) 2x 32-channel BrainAmp MR Plus 1x 16-channel BrainAmp ExG MR Cabin 2 (right) 2x 32-channel BrainAmp DC 1x 16-channel BrainAmp ExG MR The additional Brain Products peripherals available are: 2x StimTrak 2x Photodiode 1x Acoustical Stimulator Adapter 2x GSR (Galvanic Skin Response) sensor Eye Tracker SR Research EyeLink 1000 TODO: discuss camera speed, mount, etc Current Designs A Current Designs 932 unit is connected to the Presentation Computer of the right cabin. The following MR safe peripherals are available: Tethyx Joystick Bimanual Grip Force Pain Stimulation Two Digitimer current stimulators are available, the DS5 and DS7A . In practice, however, only the DS7A is used, as the DS5 is unable to provide enough current to be aversive enough for subjects. The DS7A pulse can be triggered by software, but the intensity cannot be controlled by software. Thus, the initial calibration must be done manually. Speakers Avantone Passive MixCubes were selected because they have a well understood sonic profile (modeled after the well known Auratones) and an overall linear frequency response. They are also shielded, to prevent interference. KVM Matrix The Gefen EXT-DVIKVM-444DL KVM Matrix ( datasheet ; manual ) is the key piece that makes it possible to use all the computers, screens, USB peripherals, and audio devices — in all the needed combinations. It is also a DVI amplifier, enabling reliable signal for the longer cable runs into the EEG cabins. Each matrix can connect to 4 computers (1x DVI; 1x USB; 1x 3.5mm audio) and 4 control stations (1x screen; 1x keyboard; 1x speakers/headphones). Any control station can connect to any computer — and even multiple control stations can be connected to the same computer. To demonstrate how this is useful, the following is a common experiment workflow: EEG Recorder is displayed inside the cabin: as the technician flows the gel into the caps, they need to see which ones have good contact Eye Tracker is displayed inside the cabin: to run the calibration Presentation Machine (or Researcher's Laptop) is displayed inside the cabin: to display the actual experiment Each control station has a Perixx keyboard (Periboard 409) that has 2 additional USB ports. One is used for a mouse; the other can be used by other USB response devices (joystick, etc). These are all routed, via the Matrix, to the appropriate computer. Monitors The colored dots on the monitors match the Gefen matrix remote. Recording Monitor (Green/Input #1) Samsung SyncMaster SA450 The vertical orientation allows viewing many EEG channels simultaneously. Eye Tracking Monitor (Brown/Input #2) Requires a 4:3 ratio monitor Presentation Monitor (Blue/Input #3) Samsung SyncMaster 2233RZ In-Cabin Monitor (Red/Input #4) Samsung SyncMaster 2233RZ The Samsung 2233RZ was specifically chosen because of a paper that measured its timing and found it to be favorable. TODO: link to 2233RZ paper TODO: explain refresh rate vs resolution and matrix Peripherals The following additional peripherals are available: 2x USB Joysticks (1x \"Flightstick Pro\"; 1x \"Fighterstick\") 2x 3-button 9-pin serial response box (custom) 1x 5-button 9-pin serial response box (custom) 1x 25-pin serial foot pedals (custom) TODO: scroll device Zebris TODO: Describe and link: Zebris device","tags":"labs","url":"labs/g23_r010_eeg/"},{"title":"G24-K010: Behavioral","text":"This lab is operated shared among IPSY. For questions, problems, or scheduling, contact Christina Becker or Laura Waite . Overview The lab has three open cabins, a table with a divider for head-to-head experiments, and a desk for those overseeing experiments. Software Open Cabins These 3 computers all run Debian Jessie (8). All machines have Matlab (2012b), PsychoPy, and Psychtoolbox installed. All machines have been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. Because of Matlab's network licensing, each machine is connected to the network. Head-to-Head These 2 computers run Windows 7 and have been configured so all non-essential services are disabled. Both machines have Presentation (multiple versions) installed — and also BrainVision Analyzer 2 (for instruction purposes). Since they are physically close to each other, both screens can easily be connected to the same computer for two-person experiments. Monitors Open Cabins Each cabin is equipped with a Samsung S24C450 Head-to-Head These are non-standard monitors.","tags":"labs","url":"labs/g24_k010_behavioral/"},{"title":"Data on Medusa","text":"Folder Hierarchy All data in the /home directory is available across the entire cluster. /home/<user_name> This directory is for all of your personal files. /home/data/<project_name> This directory is for data shared across the group/project. /home/<user_name>/scratch or /home/data/<project_name>/scratch This directory is not backed-up and should be used to store interim results which can be easily regenerated. Storing data here helps relieve the burden on backups. /home/data/archive/<project_name> Read-only and heavily compressed (via cool transparent compression mojo), this directory stores data for projects which are completed. If you need to archive data, contact Alex and he'll create a project folder for your data. The \"Ideal\" Data Layout This certainly won't apply to everyone in every situation, but as a base example for how a \"typical\" workflow could work. /home/rockstar Other than a few private scripts, /home/rockstar doesn't contain much data. Ms. Rockstar is a sociable scientist and doesn't slave away in the darkness; she loves the light and actively collaborates with others in her lab. /home/data/hasselhoff \"Hasselhoff\" is the name of her project, so she kindly asks Alex to setup a dedicated folder for her project. Alex creates the following folders: /home/data/hasselhoff /home/data/hasselhoff/RAW /home/data/hasselhoff/scratch The RAW folder is for the EEG and 3T data she is collecting. Alex setup that directory so that everything placed within it is automatically, transparently, and magically compressed. Once she is done collecting data, she notifies Alex and he marks the RAW folder as readonly to prevent accidental modifications of this data. Ms. Rockstar and her colleagues create whatever directory structure makes sense to them under the /home/data/hasselhoff folder. /home/data/hasselhoff/scratch Project Hasselhoff is a complex project, not to be taken lightly. It involves many steps and generates quite a bit of data. However, these interim data-sets, though numerous, are easily regenerated. This is because Ms. Rockstar scripts all of her analysis steps and submits those scripts using Condor . Thus, these interim, data-heavy steps are stored in the scratch folder to alleviate stress on the backups. /home/data/archive/psyinf/hasselhoff The project is completed, broke new ground, led to three articles published in Nature, and the first Fields Medal ever awarded to a neuroscientist. Ms. Rockstar asks Alex to create an archive folder for her. He promptly complies, and she moves the relevant, well-organized data, code, and documentation into the Hasselhoff archive folder. She then deletes any unneeded data relating to her project from her home folder, project folder, and scratch folder. She lets Alex know when she is done archiving, and he marks the archive folder as readonly. Copying to/from Medusa scp The simplest tool is scp . It works similarly to the cp command, but allows you to copy files over ssh . scp -r myanalysisfolder medusa.ovgu.de:~/analysis/ rsync Synchronizing is more powerful and efficient than copying — and rsync is the tool for syncing. I prefer it for general use, because it (by default) checksums every file transferred to guarantee that it was copied correctly. rsync -avh --progress dir_here/ medusa.ovgu.de:~/dir_there WinSCP or Cyberduck If you're on Windows (which has poor CLI support) or you simply prefer a GUI, WinSCP (Windows) and Cyberduck (macOS and Windows) are decent SFTP clients. To connect to Medusa: install and launch your client. Enter the information for host (sftp://medusa.ovgu.de), user, and password. Click connect. Now you are connected. Drag and drop data to transfer.","tags":"medusa","url":"medusa/data/"},{"title":"Backups","text":"All data located under the /home directory are snapshot at 07:00 every day — except for any data located in the /home/<user_name>/scratch or /home/data/<project_name>/scratch folders. Snapshots are then transferred to a dedicated backup server located in a different building on the OvGU campus. The backup retention policy is: daily backups kept for 2 weeks weekly backups kept for 6 weeks monthly backups kept for 1 year yearly backups kept for 2.5 years except for /home/data/archive , for which yearly backups are kept \"forever\" If you need to have data recovered from a backup, contact Alex and he will help recover your data.","tags":"medusa","url":"medusa/backups/"},{"title":"G24-K012: Eye Tracker","text":"This lab is operated by the Pollmann Lab. For questions, problems, or scheduling, contact Mikaella Sarrou . NOTE: This lab is due to be overhauled in 2018. Thus this documentation is limited — at best. Overview The lab is used primarily for behavioral and eye tracking experiments. TODO: mention back projected screen Software There are two Presentation Computers. One is a legacy, Windows XP machine (with \"some version\" of Matlab installed). The other runs a semi-standard Debian Jessie (8) setup with Matlab 2012b, PsychoPy, and Psychtoolbox installed. It has been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. There is no KVM-Matrix, so switching between the two requires that all wires be disconnected and reconnected to the desired computer. Eye Tracker SR Research EyeLink 1000 TODO: discuss camera speed, mount, etc Audio Speakers (of unknown manufacture) are available. Also available are Beyerdynamic DT-770 M headphones. Back Projected Screen TODO: Describe and link: big, back projected screen. TODO: Describe and link: projector Monitors The Presentation Monitor is a BenQ XL2410T . Peripherals The following peripherals are available: 5-button, 25-pin serial VPixx technologies ResponsePixx VPX-ACC-3000 Handheld","tags":"labs","url":"labs/g24_k012_eyetracker/"},{"title":"Condor","text":"Medusa uses HTCondor (aka: Condor) to schedule computational jobs across the cluster. While Condor has many features, one only needs to know a few core commands to begin using it effectively. Useful Commands List all slots (available and used) and their size condor_status Submit a job/job cluster condor_submit <file.submit> Summary of your jobs in the queue condor_q All of your running jobs and which machine they are on condor_q -nobatch -run All jobs from all users in the queue condor_q -nobatch -allusers Explain why a job is in a particular state condor_q -better-analyze <jobid> Remove jobs from the queue condor_rm <username> # remove all jobs for this (your) user condor_rm <clusterid> # remove all jobs belonging to this cluster condor_rm <clusterid>.<jobid> # remove this specific job User statistics, including priority condor_userprio --allusers For those who are more familiar with Sun's GridEngine, condor provides condor_qsub . condor_qsub Documentation The official Condor documentation is long, but comprehensive. If you have questions, their docs are a great resource. Pay special attention to sections 2.4, 2.5, and 2.6 of the chapter entitled Condor User Guide . The .submit File A .submit file describes the jobs (commands and their arguments) that condor will run, the environment they will run in, and the needed hardware resources (RAM, CPU). We'll start with a short, functioning example, and then each part will be explained. # The environment universe = vanilla getenv = True request_cpus = 1 request_memory = 4000 # Execution initial_dir = /home/user_bob/ executable = hello_world.sh # Job 1 arguments = \"arg1\" \"arg2\" log = /home/user_bob/logs/$(Cluster).$(Process).log output = /home/user_bob/logs/$(Cluster).$(Process).out error = /home/user_bob/logs/$(Cluster).$(Process).err Queue # Job 2 arguments = \"arg1\" \"arg2\" log = /home/user_bob/logs/$(Cluster).$(Process).log output = /home/user_bob/logs/$(Cluster).$(Process).out error = /home/user_bob/logs/$(Cluster).$(Process).err Queue The first two lines you likely will never need to change. universe declares the type of condor environment used, and getenv tells condor to copy environmental variables from your execution environment to the compute nodes. Unless you really know what you're doing, we recommend keeping these lines unchanged. universe = vanilla getenv = True Declaring resource needs is straightforward; though do note that request_memory is in MB. If you are unsure about how much memory to request: make an educated guess, submit one job, and then check its .log file, which will contain information about memory usage while the job was running. request_cpus = 1 request_memory = 4000 initial_dir is the directory that condor will cd to when starting your job. If you're using relative paths in your .submit or in scripts executed by your job, those paths are relative to this starting directory. initial_dir = /home/user_bob Declaring the executable (the program or script to be run) and the arguments to be passed to it (such as feature flags, subject data, etc) is also straightforward. executable = hello_world.sh arguments = \"arg1\" \"arg2\" Condor can generate three different types of logs per job. The log file contains information about the job — such as duration, memory usage, the node it ran on, etc. Any output that the executable prints will be recorded in the output (stdout) and error (stderr) files. The $(Cluster) and $(Process) macros supply the job ID, and are used here to create unique log files for each job. log = /home/user_bob/log/$(Cluster).$(Process).log output = /home/user_bob/log/$(Cluster).$(Process).out error = /home/user_bob/log/$(Cluster).$(Process).err The last line tells condor to schedule a job using the current state of all attributes thus far defined: Queue Then, you can change (or add) any attributes (though usually just arguments , log , output , and error ) and then add Queue again. This way, you can easily define thousands of similar jobs. TODO: Use a simple system utility as the executable, so that this example runs out-of-the-box with no need to write a hello_world.sh. Generating a .submit File Writing .submit files by hand is painful, error-prone, and does not scale — and the entire purpose of cluster computing is scale. Thus, normal operation is to have a script generate your .submit file for you. The following example shell script does the following: creates a folder for log files prints to the screen the contents for a .submit file, including: the condor environment the amount of CPU and RAM needed the script to run (analysis.py) loops over all subject csv files, scheduling one job for each file and defining unique log files for each #!/bin/sh # v1.1 cpu=1 # CPU cores needed mem=4000 # expected memory usage main_dir=/home/user_bob/tasty_Py # path to project directory log_dir=${main_dir}/logs # log path subjects_dir=${main_dir}/subjects # path to the subject files analysis_script=${main_dir}/code/analysis.py # check if the subjects directory exists; otherwise exit [ ! -d \"$subjects_dir\" ] && echo \"subject dir '$subjects_dir' not found. Exiting\" && exit 1 # create the logs dir if it doesn't exist [ ! -d \"$log_dir\" ] && mkdir -p \"$log_dir\" # print the .submit header printf \"# The environment universe = vanilla getenv = True request_cpus = $cpu request_memory = $mem # Execution initial_dir = $main_dir executable = $analysis_script \\n\" # create a job for each subject file for file in ${subjects_dir}/sub*.csv ; do subject=${file##*/} printf \"arguments = ${file}\\n\" printf \"log = ${log_dir}/\\$(Cluster).\\$(Process).${subject}.log\\n\" printf \"output = ${log_dir}/\\$(Cluster).\\$(Process).${subject}.out\\n\" printf \"error = ${log_dir}/\\$(Cluster).\\$(Process).${subject}.err\\n\" printf \"Queue\\n\\n\" done First, run the script and make sure that the output looks sane (if it fails with \"permission denied\", you probably forgot to mark it as executable by using chmod +x ). ./condor_submit_gen.sh If everything looks good, then it's time to submit the jobs to condor. The script's output can be redirected into a file using > ./condor_submit_gen.sh > the.submit condor_submit the.submit or directly to condor_submit by using | . ./condor_submit_gen.sh | condor_submit Prioritization of Jobs Condor on Medusa is configured to assess user priority when jobs are starting. The more compute resources consumed by the user, the more their priority is punished (increased). This \"punishment\" decays back to normal over the course of a day or two. In practice, it works like this: Julie submits 10,000 jobs, each ~1 hour long A day later, Jimbo submits 10 jobs Jimbo's jobs wait in the queue As some of Julie's jobs finish, resources are freed up Both Julie's and Jimbo's jobs compete for the free resources. Jimbo's win because his priority is low (good) and hers is very high (bad). There is also the Priority Factor . Users who are not members of IPSY have a modifier that punishes them even more. This way, in most cases, the jobs of IPSY members will be preferred over those of non-IPSY users. Slots Medusa is configured to allow a diversity of different job sizes, while protecting against large jobs swamping the entire cluster — and also encouraging users to break their analysis into smaller steps. The slots on Medusa are: 16x 1 cpu, 4 GiB ( 4.0 GiB/cpu) 16x 1 cpu, 6 GiB ( 6.0 GiB/cpu) 12x 1 cpu, 5 GiB ( 5.0 GiB/cpu) 6x 10 cpu, 85 GiB ( 8.5 GiB/cpu) 2x 16 cpu, 255 GiB (15.9 GiB/cpu) 1x 48 cpu, 190 GiB ( 3.9 GiB/cpu) 1x 20 cpu, 95 GiB ( 4.7 GiB/cpu) 1x 16 cpu, 415 GiB (25.9 GiB/cpu) 1x 8 cpu, 62 GiB ( 7.7 GiB/cpu) 1x 4 cpu, 18 GiB ( 4.5 GiB/cpu) All slots larger than 1 CPU are partitionable — and thus can be broken into many smaller slots. To illustrate: there are only 44x 1 CPU slots. But if 500x [1 CPU × 4 GiB] jobs are submitted, all of the larger slots are broken up into matching [1 CPU × 4 GiB] slots — resulting in a total of 231 jobs. The reader may have noticed that there are 232 CPUs, and yet only 231 jobs would be scheduled. This is because the [48 CPU × 190 GiB] slot (which has a RAM/CPU ratio < 4 GiB) cannot provide 4 GiB to each CPU; thus, one CPU is left idle. The loss of 1 CPU for [1 CPU × 4 GiB] jobs is negligible. However, as an exercise, the reader is encouraged to determine how much of the cluster would be left idle when submitting [1 CPU × 5 GiB] jobs — and also [2 CPU × 20 GiB]. The \"Ideal\" Job The \"ideal\" job is [1 CPU × 4 GiB] and runs for 10-60 minutes. Of course, not every analysis/step can be broken down into sub-jobs that match this ideal. But experience has shown that, with a little effort, the majority of analysis at IPSY can. The previous section (about slot sizes) neatly demonstrates why smaller jobs are good: simply, they are more granular and thus better fit (Tetris style) into the available compute resources. The second characteristic, duration, directly affects the turnover of jobs and how frequently compute resources become available. If 10,000x 1 hour jobs are submitted, after awhile, a job will be finishing every minute or so (due to normal variations across the cluster). Maintaining liquidity (aka job turnover) is critical for user priority to remain relevant (as discussed in the section Prioritization of Jobs) and ensure the fair-distribution-of and timely-access-to compute resources — rather than merely rewarding those who submit jobs first. 1,000 jobs lasting 1 hour each is far better than 100 jobs lasting 10 hours each. Interactive If you need more CPU or RAM than is available on the head node, you can use Condor to gain access to an interactive shell on a node — even with a GUI. condor_submit -interactive your.submit FSL FSL has been modified to directly support Condor — without the need for a submit file. When running FSL on the head node, you can set the following environmental variable to submit FSL computation directly to condor. FSLPARALLEL=condor TODO: Once compute nodes can submit jobs, this needs to be better explained and carefully reworded. However, feat does not parallelize the first level analysis. Thus, it is better to create a .submit file (or a script which generates one) to queue each feat call. The following shell script is a good starting point to generate such a .submit file. #!/bin/sh # v2.3 . /etc/fsl/fsl.sh # setup FSL environment unset FSLPARALLEL # disable built-in FSL parallelization cpu=1 # CPU cores needed mem=4000 # expected memory usage current_dir=$(pwd) # path to current working directory log_dir=\"${current_dir}/log\" # log path fsf_dir=\"${current_dir}/fsf\" # path to the .fsf files feat_cmd=$(which feat) # path to the feat command # check if the subjects directory exists; otherwise exit [ ! -d \"$fsf_dir\" ] && echo \"fsf dir '$fsf_dir' not found. Exiting\" && exit 1 # create the logs dir if it doesn't exist [ ! -d \"$log_dir\" ] && mkdir -p \"$log_dir\" # print header printf \"# The environment universe = vanilla getenv = True request_cpus = $cpu request_memory = $mem # Execution initialdir = $current_dir executable = $feat_cmd \\n\" # create a queue with each fsf file found in the current directory for fsf in ${fsf_dir}/*.fsf ; do c_basename=`basename \"$fsf\"` c_stem=${c_basename%.fsf} printf \"arguments = ${fsf}\\n\" printf \"log = ${log_dir}/\\$(Cluster).\\$(Process).${c_stem}.log\\n\" printf \"output = ${log_dir}/\\$(Cluster).\\$(Process).${c_stem}.out\\n\" printf \"error = ${log_dir}/\\$(Cluster).\\$(Process).${c_stem}.err\\n\" printf \"Queue\\n\\n\" done The script assumes that all .fsf files for each first level analysis are stored in a directory called fsf/ located under your current directory. The script will output everything to the screen, which can be piped right into condor_submit . ./fsf_submit.sh | condor_submit Python The following is an example .submit file to call a Python script. universe = vanilla getenv = True request_cpus = 1 request_memory = 4000 environment = PYTHONPATH=/usr/lib/python2.7 initialdir = /home/user_bob/Tasty_Py executable = /usr/bin/python arguments = /home/user_bob/Tasty_Py/wow.py \"arg1\" \"arg2\" log = /home/user_bob/Tasty_Py/log/$(Cluster).$(Process).subj1.log output = /home/user_bob/Tasty_Py/log/$(Cluster).$(Process).subj1.out error = /home/user_bob/Tasty_Py/log/$(Cluster).$(Process).subj1.err Queue TODO: discuss NiPype Matlab The following is an example .submit file to call Matlab universe = vanilla getenv = True request_cpus = 1 request_memory = 8000 initialdir = /home/user_bob/Wicked_Analysis executable = /usr/bin/matlab arguments = -singleCompThread -r Gravity(1) log = /home/user_bob/Wicked_Analysis/log/$(Cluster).$(Process).subj1.log output = /home/user_bob/Wicked_Analysis/log/$(Cluster).$(Process).subj1.out error = /home/user_bob/Wicked_Analysis/log/$(Cluster).$(Process).subj1.err Queue By default, Matlab will use all available CPUs. The only effective way to control Matlab is to use the -singleCompthread option, which will limit it to a single CPU. There is a maxNumCompThreads() function, but it is deprecated and is considered unreliable. NOTE: With the increase in the number of available campus toolbox licenses, it is no longer necessary to restrict Matlab jobs to specific compute nodes. OpenBlas OpenBlas automatically scales up to use all the CPUs on a system. For example, to limit it two CPUs, set the following environmental variable. OMP_NUM_THREADS=2 DAGMan TODO: discuss DAGMan Intel vs AMD Our cluster's Intel nodes have the fastest single thread performance. If you have very few, single CPU jobs and need them to execute as fast as possible, then restricting your jobs to the nodes with Intel CPUs can be beneficial. The nodes are configured to advertise their CPU vendor, so it is easy to constrain according to CPU type. Add the following to your .submit file. Requirements = CPUVendor == \"INTEL\" Or, to prefer Intel CPUs but not require them Rank = CPUVendor == \"INTEL\"","tags":"medusa","url":"medusa/condor/"},{"title":"G24-K013: Soundproof","text":"This lab is operated by the Pollmann Lab. For questions, problems, or scheduling, contact Mikaella Sarrou . Overview The lab has one soundproof lab for eye tracking experiments. It is also equipped with a KVM matrix, which allows for nearly complete flexibility over which computer (EEG recorder, Eye Tracker, Presentation Machine, or researcher supplied Laptop) displays to any screen — and routes both audio and USB. Software The Presentation Computer runs Debian Jessie (8) with Matlab (2012b), PsychoPy, and Psychtoolbox installed. It has been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. Eye Tracker SR Research EyeLink 1000 TODO: discuss camera speed, mount, etc Communication with the eye tracker is done via the yellow Ethernet cable. The Presentation computer is already setup. If you need to communicate using your laptop, use the following settings to configure your NIC: Address: 100.1.1.2 Netmask: 255.255.255.0 Gateway: 100.1.1.1 Speakers A portable, USB-rechargable, battery operated speaker is available. KVM Matrix The Gefen EXT-DVIKVM-444DL KVM Matrix ( datasheet ; manual ) is the key piece that makes it possible to use all the computers, screens, USB peripherals, and audio devices — in all the needed combinations. It is also a DVI amplifier, enabling reliable signal for the longer cable runs into the EEG cabin. The matrix can connect to 4 computers (1x DVI; 1x USB; 1x 3.5mm audio) and 4 control stations (1x screen; 1x keyboard; 1x speakers/headphones). Any control station can connect to any computer — and even multiple control stations can be connected to the same computer. To demonstrate how this is useful, the following is a common experiment workflow: Eye Tracker is displayed inside the cabin: to run the calibration Presentation Machine (or Researcher's Laptop) is displayed inside the cabin: to display the actual experiment Each control station has a Perixx keyboard (Periboard 409) that has 2 additional USB ports. One is used for a mouse; the other can be used by other USB response devices (joystick, etc). These are all routed, via the Matrix, to the appropriate computer. Monitors The colored dots on the monitors match the Gefen matrix remote. Presentation Monitor (Green/Input #1) iiyama G-MASTER GB2488HSU-B2 Eye Tracking Monitor (Brown/Input #2) Requires a 4:3 ratio monitor In-Cabin Monitor (Red/Input #4) iiyama ProLite GB2488HSU-B1 The In-Cabin Monitor is wall mounted with an adjustable arm and swivel. This allows the monitor height and depth to be adjusted for experiments which require different visual-fields. A laser distance meter and level is available (on the shelf near the doorway) to verify that the screen is positioned correctly. TODO: explain refresh rate vs resolution and matrix Peripherals The following peripherals are available: 8-button, USB Cedrus RB-830 Response Pad 5-button, 25-pin serial Psychology Software Tools 200A Response Box 6-button, 25-pin serial Cedrus RB-600 Response Box","tags":"labs","url":"labs/g24_k013_soundproof/"},{"title":"EXFA: Skyra","text":"NOTE: This documentation is in progress. Overview TODO: add contact info Software There is one presentation computer, provided by the biopsych lab. It runs both Windows XP and Debian Jessie. Debian This is the standard experiment machine setup. Matlab (2012b), PsychoPy, and Psychtoolbox are installed. The machine has been configured to match the timing/configuration requirements of both PsychoPy and Psychtoolbox. Windows An ancient Windows XP install. It is, for obvious reasons, deprecated, but it is functional. Presentation is installed. Scanner 3 Telsa Siemens Magnetom Skyra TODO: discuss scanner triggers Projector TODO: Describe and link: projector Peripherals TODO: Describe parallell response box","tags":"labs","url":"labs/EXFA_skyra/"},{"title":"Software","text":"A wide variety of software is installed across the cluster. The majority are preconfigured, but some require additional setup by each user before they can be used. FSL FSL ships with many commands which conflict with other command names, so all FSL commands are prepended with fsl5.0- . To configure FSL with defaults and remove the fsl5.0- prefix, run the following: . /etc/fsl/fsl.sh This needs to be run each time you start a new session on Medusa. If you'd rather it be done automatically, add it to your .bashrc file. FreeSurfer FreeSurfer's configuration is setup using something called \"environmental modules.\" First, \"modules\" must be loaded: . /etc/profile.d/modules.sh Then, you can query which versions of FreeSurfer are available: module avail freesurfer Then, to load version 6.0, run the following: module load freesurfer/6.0 Now you can use FreeSurfer. The process needs to be performed each time you start a new session on Medusa. If you'd rather it be done automatically, add the following lines to your .bashrc file. . /etc/profile.d/modules.sh module load freesurfer/6.0","tags":"medusa","url":"medusa/software/"},{"title":"Data Center","text":"We have 5U in one of the racks in the G26 data center. It houses the backup server (Thunk). PWR U# Name Inventory # Overview 4 JBOD (Thunk) (likely part of 264097,000) 9x 4TB drives / 12x bays 5 6 Thunk 261309,000 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM 12x 4TB drives / 16x bays 7 8 In the G01 data center, we own a 42U rack. The contents of the rack are (as of 2018.08.09): PWR U# Name Inventory # Overview 1 UPS 243181,000 1000 VA 2 UPS 3 Router 1019378 SG-8860 UPS 4 DMZ Switch 48 Port HP V1910-48G 5 Cable Management UPS 6 Mgmt. Switch 24 Port HP ProCurve 1700-24 7 Cable Management UPS 8 Data Switch 1019379 28 Port Netgear XS728T 9 10 Mulder 260876,000 1x 4-core 3.2 GHz Xeon E3-1230 16 GiB RAM 11 12 13 14 15 16 17 18 19 20 21 N/I 22 Sunshine 252757,000 Sun Fire T2000 1x 8-core 1.2GHz 16 GiB RAM 23 24 snake12 264097,000 4x 8-core 2.6 GHz Opteron 6212 512 GiB RAM 25 snake11 1019380,000 2x 10-core 2.3 GHz Xeon E5-2650v3 96 GiB DDR4 RAM 26 27 snake10 267616,000 4x 8-core 2.8 GHz Opteron 6320 512 GiB RAM 28 snake9 246831,000 1x 8-core 2.0 GHz Opteron 6128 64 GiB RAM 29 snake8 245075,000 1x 4-core 2.67 GHz i7 920 18 GiB RAM 30 31 snake7 4x 16-core 2.4 GHz Opteron 6272 256 GiB RAM 32 snake5 & snake6 261309,000 (guessing) each snake: 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM 33 snake3 & snake4 34 snake1 & snake2 PDU & UPS 35 Medusa 265021,000 4x 8-core 2.8 GHz Opteron 6320 256 GiB RAM 36 PDU & UPS 37 Zing 1031333 13x 3.84TiB SanDisk Optimus 2 Max / 24x bays 38 PDU & UPS 39 JBOD (Zing) 265526,000 6x 4TB HDD / 12x bays 40 41 UPS 3000 VA 42 The rack's Inventory Nr.: 248252,00 Legend: N/I: not installed (physically not in the rack) N/C: not connected (physically in the rack)","tags":"medusa","url":"medusa/data_center/"},{"title":"Hardware","text":"Summary As of December 2017, the cluster comprises 16 nodes with nearly 300 CPU cores and 2.5 TiB of RAM. Centralized storage features more than 40 TiB of high performance SSD and 11 TiB of HDD capacity, and is accessed by cluster nodes via 10Gb Ethernet. Head Node (Medusa) 4x 8-core 2.8 GHz Opteron 6320 256 GiB RAM (16x 16GiB DDR3 ECC reg) 1x 10Gb NIC Purchased 2013.12. Supermicro's specs: A+ Server 2042G-TRF . Data Node (Zing) 1x 8-core 3.2 GHz Xeon E5-1660 v4 96 GiB RAM (6x 16GB DDR4 2133 ECC reg) 42 TiB SSD (~29.3 TiB usable) and ~26 TiB HDD (~10.5 TiB usable) storage 2x 10Gb bonded NICs Purchased in 2016.12. Supermicro's specs: SuperChassis 216BE1C-R920LPB and Mainboard X10SRL-F Backup Node (Thunk) The only server in this list that is not hosted in G01. It is instead across campus in the G26 data center. 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM (12x 8GB DDR3 ECC reg) 76.4 TiB HDD (52.1 TiB usable) storage 1x 1Gb NIC Purchased 2011.12. Supermicro's specs: SuperChassis 836E16-R1200B and Mainboard X8DTH-iF snake1-6 2x 6-core 2.4 GHz Xeon E5645 96 GiB RAM (12x 8 GiB DDR3 ECC reg) 1x 1Gb NIC Purchased 2011.12. Supermicro's specs: Twinserver 6016TT-TF snake7 4x 16-core 2.4 GHz Opteron 6272 256 GiB RAM (32x 8 GiB DDR3 ECC reg) 1x 1Gb NIC Purchased 2012.06. Supermicro's specs: H8QG6+-F Motherboard snake8 1x 4-core 2.67 GHz i7 920 18 GiB RAM 1x 1Gb NIC Purchased 2010.03; formerly amras. Supermicro's specs: RM21706 Chassis and X8STE Motherboard snake9 1x 8-core 2.0 GHz Opteron 6128 64 GiB RAM (8x 8GiB DDR3 ECC Reg) 1x 1Gb NIC Purchased 2010 (estimated); formerly \"just laying about\" in Toemme's lab. Supermicro's specs: H8DGU-F Motherboard snake10 4x 8-core 2.8 GHz Opteron 6320 512 GiB RAM (32x 16GiB DDR3 ECC Reg) 1x 1Gb NIC Purchased 2013.12. Supermicro's specs: A+ Server 1042G-TF snake11 2x 10-core 2.3 GHz Intel Xeon E5-2650v3 96 GiB RAM (6x 16GiB DDR4 ECC Reg) 1.2 TB NVMe mounted at /tmp 1x 10Gb NIC Purchased 2015.12. Supermicro's specs: 825TQ-R740LPB Chassis and X10DRi-T Motherboard snake12 4x 8-core 2.6 GHz Opteron 6212 512 GiB RAM (32x 16GB DDR3 ECC reg) 1x 1Gb NIC Purchased 2012.12; formerly mudflap. Supermicro's specs: A+ Server 1042G-TF and SuperChassis 826E16-R1200LPB Mulder TODO: Add Mulder's specs Networking An SG-8860 running OpenBSD acts as the router/firewall/etc for the cluster. All cluster traffic uses one 1Gb connection. All web services use a separate 1Gb connection. Internally, the data network is 10Gb (though not all hosts have 10Gb cards). Both the management and DMZ networks are physically separate. Power We have one zero-U 3-phase PDU ( Raritan PX2-2730 ). It is connected to a red IEC_60309 power plug. For specifics on which machine is plugged into which PDU outlet, consult the rack diagram . As we have limited battery capacity, only critical equipment is protected by the UPSs. Both UPSs are monitored by zing via NUT ; the head node and zing poll this information; if the main UPS (Eaton) reaches low battery , both Medusa and Zing will shutdown immediately. APC Smart-UPS SC 1000 age: ~2009 protects router and switches Eaton 5PX 2200 age: 2012.12 protects Medusa and Zing","tags":"medusa","url":"medusa/hardware/"},{"title":"Tools","text":"You can do amazing things when you know your tools. Your browser does not support the video tag.","tags":"pages","url":"tools/"},{"title":"The Command Line","text":"The shell (sometimes also called a terminal, console, or CLI) is an interactive, text based interface. If you have used Matlab or IPython, then you are already familiar with the basics of a command line interface. While the initial learning curve is steeper for the command line, the rewards are well worth it. Command line programs tend to be faster, more flexible, and more scalable than their GUI counterparts. Syntax Commands are case sensitive and follow the syntax of: command [options...] <arguments...> . The options modify the behavior of the program, and are usually preceded by - or -- . For example: ls -l test.txt ls is the command . The option -l tells ls to display more information. test.txt is the argument — the file that ls is listing. Every command has many options (often called \"flags\") that modify their behavior. There are too many to even consider memorizing. Remember the ones you use often, and the rest you will lookup in their documentation or via your favorite search engine. Basic Commands pwd print the name of the folder you're currently in ls -lah <folder> list the contents of a folder, including hidden files ( -a ), and all their information ( -l ); print file sizes in human readable units ( -h ) [1] cd <folder> change to another folder cp <from> <to> copy a file cp -R <from> <to> copy a folder and its contents ( -R ) mv <from> <to> move/rename a file or folder rm <file> delete a file rm -Rv <folder> delete a folder and its contents ( -R ) and list each file as it's being deleted ( -v ) mkdir <folder> create a folder rmdir <folder> delete an empty folder chmod -R g+rwX <folder> give group members ( g+ ) read, write ( rw ), and execute if already present for others ( X ) permissions for a folder and all of its contents ( -R ); see the Section on Permissions for more info. chown -R <username> <folder> change the owner of a folder and all of its contents ( -R ); see the Section on Permissions for more info. chgrp -R <groupname> <folder> change the group of a folder and all of its contents ( -R ); see the Section on Permissions for more info. echo \"text\" print text to the screen Redirection Now that you know some of the basic shell commands, it's time to introduce some core shell concepts. The output that commands print to the screen can also be redirected into a file or used as the input to an another program. > > writes the output of a command to a file. If the file already exists, it will overwrite the contents. For example: echo \"What did the llama say when asked to go on a picnic?\" > lame_joke.txt Or, more practically, the output of a long running search. find /home/data/psyinf -t f -name \"\\*.fsf\" -print > ffs_these_fsf.txt >> >> appends the output to a file. If the file doesn't exist, it will create it. echo \"Great! Alpaca lunch!\" >> lame_joke.txt | | (pipe) redirects the output of a command and uses it as the input for the next command. For example, the following will send the output of echo to sed , which replaces \"fellow\" with \"good looking\". echo \"hello there, fellow\" | sed \"s/fellow/good looking/\" More practically, the following command calculates the size of each file and folder in /tmp . The output is then sorted by size. du -sh /tmp/* | sort -h The Prompt When you first login on the command line, you are greeted with \"the prompt\", and it will likely look similar to this: aqw@medusa:~$ This says I am the user aqw on the machine medusa and I am in the folder ~ , which is shorthand for the current user's home folder (in this case /home/aqw ). The $ sign indicates that the prompt is interactive and awaiting user input. [2] In documentation, $ is commonly used as a shorthand for the prompt, and allows the reader to quickly differentiate between lines containing commands vs the output of those commands. For example: $ ls -la wombats.txt -rw-rw---- 1 aqw psyinf 6 Nov 29 10:00 wombats.txt Paths Let's say I want to create a new folder in my home folder, I can run the following command: mkdir /home/aqw/awesome_project And that works. /home/aqw/awesome_project is what is called an absolute path. Absolute paths always start with a / , and define the folder's location with no ambiguity. However, much like in spoken language, using someone's full proper name every time would be exhausting , and thus pronouns are used. This shorthand is called relative paths, because they are defined (wait for it...) relative to your current location on the file system. Relative paths never start with a / . . the current directory .. the parent directory ~ the current user's home directory So, taking the above example again: given that I am in my home folder, the following commands all would create the new folder in the exact same place. mkdir /home/aqw/awesome_project mkdir ~/awesome_project mkdir awesome_project mkdir ./awesome_project To demonstrate this further, consider the following: In my home directory /home/aqw I have added a folder for my current project, awesome_project/ . Let's take a look at how this folder is organized: └── home └── aqw └── awesome_project ├── aligned ├── code └── sub-01 └── bold3T └── sub-02 └── bold3T ├── ... └── sub-xx └── bold3T └── structural └── sub-01 └── anat └── sub-02 └── anat ├── ... └── sub-xx └── anat Now let's say I want to change from my home directory /home/aqw into the code/ folder of the project. I could use absolute paths: cd /home/aqw/awesome_project/aligned/code But that is a bit wordy. It is much easier with a relative path: cd awesome_project/aligned/code Relative to my starting location ( /home/aqw ), I navigated into the subfolders. I can change back to my home directory also with a relative path: cd ../../../ The first ../ takes me from code/ to its parent aligned/ , the second ../ to awesome_project/ , and the last ../ back to my home directory aqw/ . However, since I want to go back to my home folder, it's much faster to run: cd ~ Globbing Most modern shells have powerful pattern matching abilities (often called globbing) that allows you to match the names of multiple files and/or directories. This is especially useful when running a command on many files at once. When globbing, the shell compares the pattern to files on the file system and expands the term to all matching file names. The most basic pattern is * , which matches any number of any character(s). For example, the following will list all files in the current directing ending in .txt : ls *.txt Or, lets you move a bunch of .jpg files into a folder: mv -v *.jpg annoying_instagram_food_pics/ Globbing can also nest through directories. For example, assuming a typical folder structure for subject data, you can list every subject's functional .nii.gz files for run 1: ls sub-*/func/*_run-1_*.nii.gz You can read about more about Pattern Matching in Bash's Docs . Permissions Every file and folder has permissions which determine which users are allowed to read, write, and execute it. $ ls -la wombats.txt -rw-rw---- 1 aqw psyinf 6 Nov 29 10:00 wombats.txt The -rw-rw---- provides all the information about this file's permissions. The left-most - indicates whether it's a file, a folder ( d ), a symlink ( l ), etc. The rest are three tuplets of --- . The first tuplet is for the user, the second tuplet is for the group, the last tuplet is for all other users. The above example shows that both the user ( aqw ) and the group ( psyinf ) have read and write permissions ( rw- ) to wombats.txt . All other users on the system have no permissions ( --- ). Let's say I don't want others in the psyinf group to have permission to write to wombats.txt anymore. $ chmod g-w wombats.txt $ ls -lah wombats.txt -rw-r----- 1 aqw psyinf 6 Nov 29 10:00 wombats.txt TODO: explain chmod 640 vs chmod g-w TODO: discuss (and show how to set UMASK) TODO: discuss user-private groups, sticky bit TODO: point to a more exhaustive explanation and/or man page Useful Commands man <command_name> show the manual (documentation) for a command ssh <username>@<servername> log into an interactive shell on another machine passwd change your password rsync -avh --progress from_folder/ <user>@<server>:/destination/folder sync/copy from a local folder to a folder on a remote server via SSH. Will preserve all permissions, checksum all transfers, and display its progress. grep -Ri <term> <folder> case-insensitive search for a term for all files under a folder htop overview of computer's CPU/RAM and running processes pip install --user <python_pip_package> install Python packages into your home folder sed -i \"s/oops/fixed/g\" <file> replace all occurrences of 'oops' with 'fixed' in a file wget <link> download a file find <folder> -type d -exec chmod g+s {} \\; find all folders underneath a directory and apply the \"sticky bit\" to them; see the Section on Permissions for more info. du -sh <folder> print how much disk space a folder uses cat <file> print the contents of a file to the screen head -n 20 <file> show the first 20 lines of a file tail -n 10 <file> show the last 10 lines of a file tail -f <file> print the last 10 lines of a file, and continue to print any new lines added to the file (useful for following log files) less <file> print the content of a file to the screen, one screen at a time. While cat will print the whole file, regardless of whether it fits the terminal size, less will print the first lines of a file and let you navigate forward and backward ln -s <target> <link_name> create a symlink (a shortcut) TODO: sudo TODO: unzip/tar/gzip TODO: sshfs (different section/page?) TODO: tmux (different section/page?) Piping Fun du -sh ./* | sort -h calculate the size of each of the files and folders that are children of the current folder, and then sort by size find ./ -mmin -60 | wc -l find all files under the current directory that have been modified in the last 60 minutes, and then count how many are found ls -lah ~/ | less list all files in your home folder and display them one page at a time Text Editors Text editors are a crucial tool for any Linux user. You will often find the need for one, whether it is to quickly edit a file or write a collection of analysis scripts. Religious wars have been fought over which is \"the best\" editor. From the smoldering ashes, this is the breakdown: nano Easy to use; medium features. If you don't know which to use, start with this. vim Powerful and light; lots of features and many plugins; steep learning curve. Two resources to help get the most out of vim are the vimtutor program (already installed on Medusa) and vimcasts.org . emacs Powerful; tons of features; written in Lisp; huge ecosystem; advanced learning curve. TODO: link to vim plugins; suggest minpac Shells TODO: bash TODO: zsh TODO: tab completion (gifs?) TODO: history (up and searching) TODO: perhaps link to prezto, etc Footnotes [1] By default, file sizes are printed in Bytes. The -h flag changes this to units sane for human consumption. For example: 137216 would instead be listed as 134K. And for those brains rioting right now, remember, computers are binary, so 1K is 1024 bytes (2 10 ), not 1000 (10 3 ). [2] The # symbol is commonly used to indicate a prompt with elevated permissions (such as the root user).","tags":"tools","url":"tools/cli/"},{"title":"Git","text":"Git enables you to track the changes made to files over time — specifically: what changed, by whom, when, and why. It also gives you the capability to revert files back to a previous state. Over time, as your project evolves, you can edit your files with confidence knowing that at any point you can look back and recover a previous version. Install Debian/Ubuntu sudo apt-get install git macOS Download the installer at: https://git-scm.com/download/mac Windows Download the installer at: https://git-scm.com/download/win Setup Once Git is installed, configure it with your name and email address. This lets Git know who you are so that it can associate you with the commits you make. git config --global user.name \"Wiggly McWidgit\" git config --global user.email wiggity-wiggity-wack@example.com Basic Commands git init Tells git to enable tracking of changes that happen in this folder. git clone <url> | <user@server:path/to/repo.git> Makes a full copy of an existing git repository — all files, folders, changes, history, etc. git status Lists which files are in which state — if there have been changes made, new files added or deleted, etc. git add <file> To begin tracking a new file. Once you run git add , your file will be tracked and staged to be committed. git add -p Review the changes you've made and select which will be staged . git commit Commits all the staged changes (done with git add ). It will prompt you for a commit message , which should be a terse but descriptive note about the changes contained in the commit. These commit messages are your project's history. git rm <file> Stages the file to be removed. After you commit, the file will be removed and no longer tracked. But the file does remain in the project history. git mv <file-from> <file-to> Moves/renames a file. git log Lists your commit history. It's not as user-friendly or easy-to-navigate as tig . tig A text-mode interface for git that allows you to easily browse through your commit history. It is not part of git and will need to be installed ( apt-get install tig for Debian/Ubuntu; Homebrew instructions for macOS) git push Push your local changes to another repository, for example on GitHub. git pull Pull changes from another repository to your local repository. GitHub GitHub is an online platform where you can store and share your projects; it is especially well suited for working on a project with several other people. It acts as a central place where everyone can access/contribute to the project and offers several useful tools (issues, wikis, project milestones, user management, etc) that make collaboration simple and easy. To create a profile, go to GitHub , and from there, follow the prompts to create your account. Resources GitHub offers an interactive Git tutorial that is a great starting point for beginners. The free Pro Git Book covers just about everything Git has to offer using clear and easy-to-understand language. It starts with the basics, but builds up to some of Git's more complex features. If you like video tutorials, the Intro to Git and GitHub and The Basics of Git and GitHub videos are worth watching if you're still unsure about the basics of Git and GitHub and want a step-by-step explanation of how to get started. For any questions you might have about using GitHub, see GitHub Help . The Git Reference Manual is the official docs for Git. It has all the information you could want to know about Git, but is pretty dense and better suited for intermediate and advanced users.","tags":"tools","url":"tools/git/"},{"title":"R","text":"The statistical software R is available on Medusa --- along with a wide variety of packages that are readily available to load via the library() command. However, the R ecosystem has over 12,000 packages, so naturally not all are installed on Medusa. If you need an R package installed on Medusa, I recommend you follow these steps: Debian Packages First, see if it's already packaged for Debian. Many of the most popular R packages are already packaged for Debian. If you can find the package via aptitude on Medusa (for example aptitude search tidyr ), then let Alex know and he can install the package for you. This is the preferred method, as it saves time and potential headaches. But also, packages installed using this method are widely tested and are installed for all users on Medusa. This helps maintain a common environment where scripts can be trivially shared among users with little divergence in analysis environments. User Private If the package you want installed isn't already packaged for Debian, then you can still install it, but it will only be available for your user. The installation process can range from dull, to thrilling, to aggravating beyond all belief. It will quickly become apparent which adventure you have chosen. Before you can install R packages in your home folder, you must configure it to do so. First create a directory for the packages: mkdir -p ~/.R/library Then tell R you want to use this folder: echo 'R_LIBS_USER=\"~/.R/library\"' > ~/.Renviron Any package that is available via CRAN can now be installed to ~/.R/library using ìnstall.packages() . For example: install.packages(\"ggvis\")","tags":"tools","url":"tools/R/"},{"title":"Services Provided","text":"In addition to Medusa and the Experimental Labs , the following services are provided: JupyterHub (alpha) A JupyterHub installation is available and in alpha status. Both R and Python notebooks are currently supported. If you would like an account for yourself or to teach a class, contact Alex. Note, this service is currently in Alpha and will move to new hardware sometime in 2018. Mailing Lists exppsy@ovgu.de Subscribe to exppsy neuropsy-list@ovgu.de Subscribe to neuropsy biopsy-l@ovgu.de Subscribe to biopsy brazi-l@ovgu An IPSY-wide mailing list that emails directly to the above three lists, plus members of other IPSY labs which don't have a mailing list. Subscribe to brazi-l Webspace Web servers are available for: Lab/Project Websites such as neuro.debian.net , studyforrest.org , howdoyouscience.net , etc Data Distribution such as psydata.ovgu.de Personal Websites More info at Transferring Data Debian Repository IPSY-specific packages (scripts, configuration) and license restricted software (Freesurfer, Matlab, etc) is available. To add the repo to your Debian machine, run the following: sudo printf \"deb http://kumo.ovgu.de/debian stretch main\" > /etc/apt/sources.list.d/ipsy.list curl http://kumo.ovgu.de/debian/ipsy_apt.gpg.key | sudo apt-key add - sudo apt-get update sudo aptitude search ipsy- Due to the restrictive licensing, this repository is only available to machines with a wired connection in the IPSY offices. OwnCloud (EOL) The OwnCloud deployment is now deprecated and no new users are permitted. Existing users are being migrated to other solutions. Seafile (coming in 2018) An alternative to DropBox that will be hosted on IPSY infra.","tags":"pages","url":"services/"},{"title":"Printing","text":"When in doubt, use the drivers bundled with your OS. Otherwise, use the links below to find the appropriate drivers. Ullsperger Lab Kyocera FS-C5400DN 141.44.98.7 Room G24-003 Pollmann Lab Kyocera ECOSYS P6035CDN 141.44.98.9 Room G24-010 Gerhard Lab Kyocera FS-C5250DN 141.44.98.12 Room G22A-329 005 MFP Brother MFC-9450CDN 141.44.98.11 Room G24-005 005 Color Kyocera FS-C5150CDN 141.44.98.13 Room G24-005 Junior XXIII Kyocera FS-C5150CDN 141.44.98.14 G23 Kitchen G24 Copier/Printer e-STUDIO 256SE 141.44.96.89 Main floor (near vending machines)","tags":"pages","url":"printing/"},{"title":"Frequently Asked Questions","text":"Backup Policy For the cluster and all servers, backups are performed daily . For laptops and desktops, there is no centralized backup process. It is your responsibility to come up with a solution that meets your needs. As examples, some users... * sync a folder to their account on Medusa * backup their machines to an external hard drive (i.e. Time Machine) * sync with DropBox/Google Drive * just don't have important data on their machines Setting Up Email The URZ has email instructions available to help you configure your email client/app. Remote Access to OvGU You will need to use VPN if you wish to access OvGU's licensing servers or journal subscriptions while off-campus. The URZ has VPN instructions available. On macOS, the built-in Cisco VPN is more convenient than the official Cisco client recommended by the URZ. Go to System Preferences -> Network Click the + symbol in the lower-left corner Interface: \"VPN\" VPN Type: \"Cisco IPSec\" Service Name: \"OvGU VPN\" Back in the main network window Server Address: \"vpn.ovgu.de\" Account Name: <your ovgu account> (the same one to authenticate for email) Click on Authentication Settings Shared Secret: \"vpn1\" Group Name: \"vpn1\" Software Licensing Martin Krippl is responsible for IPSY proprietary software licensing. He's the one to talk to if you need Office, EndNote, Matlab, SPSS, etc. Hardware Recommendations Alex Waite can help you if you have questions about the fit of a given hardware solution or need help coming up with the specs for a new hardware purchase. Once determined, you will need to work with the secretary responsible for your lab. They can best guide you through the purchasing process and inform you of any strings that may be attached with the available funds. Updating Website Content The websites for IPSY and most of its labs use OvGU's official Content Management System. Information about how to update content in their system can be found at cms.ovgu.de . If the content you want to add doesn't need to be on the main website, then perhaps your personal webspace on kumo would be a good fit.","tags":"pages","url":"faq/"},{"title":"Contributing to the Docs","text":"Found a problem or have a suggestion for how these docs can be improved? You can report it on the issues tracker on GitHub. And... while bug reports are welcome, patches are even more welcome. ;-) The git repository for this site is hosted on GitHub . If you are not already familiar with git and/or GitHub, read our git documentation first. If your proposed fix is limited to content, then you can probably skip setting up the build system. You can simply make your changes (content is written in reStructuredText ) and submit a Pull Request . The website is generated by Pelican and the template is written using Jinja2 . So, if your proposed changes are more widespread or change the template, you'll need to setup the build environment. Don't worry; it's easy. install pelican ( apt-get install pelican on Debian; pip install --user pelican on other OSs.) run make devserver open a browser to http://127.0.0.1:8000 And that's it. Any changes you make will automatically trigger a rebuild.","tags":"pages","url":"contributing/"},{"title":"News","text":"Known Issues \"Some\" .hdf5 files remain locked after initial creation. The source of this problem is elusive, and makes little sense. The current work-around is to cp the file and then mv the copy over the original. This forces the file to be assigned a new inode, which invalidates the in-file locking. Madness. Events 2018.08.31 - ZFS upgrade The data node's version of ZFS was upgraded to 0.7.9. This brings a wide range of fixes. The cluster was rebooted for the update to take effect. 2018.08.02 - Wine 3.0.1 installed cluster-wide If you really need to shoe-horn your Windows-based workflow onto our Debian cluster, then there is a small ray of hope for you. If you can get your application to run via Wine 3.0.x, you can now run it across the entire cluster. 2018.07.09 - Increased number of Matlab toolbox licenses The university has a new licensing agreement with Mathworks. In all practicality, there are now an unlimited number of Matlab and toolbox licenses (10,000). Thus, Matlab users on Condor no longer need to limit the number of compute nodes used due to licensing constraints. The Condor/Matlab documentation has been updated to reflect this. 2018.07.09 - Fixed swap vs /tmp disk allocation on compute nodes Due to a bug in the installation's preseed configuration, compute nodes with large hard drives were allocating the excess space to the swap partition rather than the /tmp directory. This has been fixed, and all nodes have been reinstalled. It is now possible for jobs to run which need a large amount of local disk space rather than NFS. 2018.07.06 - Updated nibabel, nipype, indexed-gzip, fsleyes Updated version of these packages have been installed, which should finally allow them all to coexist in fully updated harmony. Previously, many tools were displaying warnings, and a downgraded version of nibabel was needed to keep everything functional. 2018.06.13 - Signing Key for IPSY's Debian Repo Expired The signing key for IPSY repository of Debian packages on Kumo expired. It has been updated and the updated key deployed to all cluster systems. If this is affecting you on your local system, run the following: curl http://kumo.ovgu.de/debian/ipsy_apt.gpg.key | sudo apt-key add - sudo apt-get update sudo apt-get install --only-upgrade ipsy-keyring If prompted about a conflicting ipsy.gpg file, respond with Y . 2018.04.11 - Condor jobs fail to start on snake4 When jobs attempted to run on snake4, they would bounce between running and idle and complain in the logs about a \"Shadow Exception\". The cause was a deeply mangled /etc/passwd file. The node has been reinstalled. 2018.04.05 - fsleyes crashes on start An updated dependency of fsleyes caused it to crash. The bug was reported, the upstream maintainer released a fix, and that fix has now been deployed. 2018.03.16 - DataLad Upgrade DataLad was upgraded and moved from a system package to a singularity container. Most users shouldn't notice a difference, but if you were using any of its Python libraries directly, they are no longer installed system-wide. 2018.03.13 - HeuDiConv/Nipype Fixed Nipype was failing (prematurely), complaining about an outdated version of Pydot. Until the real fix is applied, an updated version of Pydot has been backported, which seems to resolve the problem. 2018.03.11 - ZFS upgrade The data node's version of ZFS was upgraded to 0.7.6. This brings a wide range of fixes, especially performance related. Hopefully this will end the elusive \"some files, take 1+ minute each to delete\" problem. The cluster was rebooted for the update to take effect.","tags":"pages","url":"news/"}]}